{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) Conjunto de 1000 datos de entrenamiento y 1000 datos de test etiquetados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import random as rd\n",
    "from numpy import exp, array, dot, random\n",
    "from random import choice\n",
    "import math as mt\n",
    "import pandas as pd\n",
    "import _pickle as pickle\n",
    "import os\n",
    "import keras\n",
    "\n",
    "#from scipy.misc import imread\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from pylab import plot, ylim\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "from top_level_features import hog_features\n",
    "from top_level_features import color_histogram_hsv\n",
    "from top_level_features import extract_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1) Escriba una función que genere (aleatoriamente) $n$ datos etiquetados de la forma $\\{(x_1,y_1),\\ldots,(x_n,y_n)\\}$ con $x_i \\in \\mathbb{R}^2$, $y_i \\in \\{0,1\\}$, con una distribución de probabilidad que reflrja la configuración linealmente inseparable que muestra la Fig. 1. Utilice está función para generar un conjunto de 1000 datos entrenamiento y 1000 datos de pruebas. El problema de clasificación obtenido se denomina en ocasiones \"XOR\". ¿Porqué?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generación de datos (training y testing sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ready\n"
     ]
    }
   ],
   "source": [
    "f = open('train.txt', 'w')\n",
    "g = open('test.txt', 'w')\n",
    "nn = 1000\n",
    "\n",
    "for i in range(0,nn):\n",
    "    x_1 = rd.uniform(-1, 1)\n",
    "    x_2 = rd.uniform(-1, 1)\n",
    "\n",
    "    while (x_1 == 0):\n",
    "        x_1 = rd.uniform(-1, 1)\n",
    "\n",
    "    while (x_2 == 0):\n",
    "        x_2 = rd.uniform(-1, 1)\n",
    "\n",
    "    if (x_1>0):\n",
    "        if(x_2>0):\n",
    "            y = 1\n",
    "        else: \n",
    "            y = 0\n",
    "    else: \n",
    "        if(x_2<0):\n",
    "            y = 1\n",
    "        else: \n",
    "            y = 0\n",
    "    #print>>f, x_1,x_2,y\n",
    "    f.write(str(x_1)+\"\\t\"+str(x_2)+\"\\t\"+str(y))\n",
    "    \n",
    "    \n",
    "for i in range(0,nn):\n",
    "    x_1 = rd.uniform(-1, 1)\n",
    "    x_2 = rd.uniform(-1, 1)\n",
    "\n",
    "    while (x_1 == 0):\n",
    "        x_1 = rd.uniform(-1, 1)\n",
    "\n",
    "    while (x_2 == 0):\n",
    "        x_2 = rd.uniform(-1, 1)\n",
    "\n",
    "    if (x_1>0):\n",
    "        if(x_2>0):\n",
    "            y = 1\n",
    "        else: \n",
    "            y = 0\n",
    "    else: \n",
    "        if(x_2<0):\n",
    "            y = 1\n",
    "        else: \n",
    "            y = 0\n",
    "    #print>>g, x_1,x_2,y\n",
    "    g.write(str(x_1)+\"\\t\"+str(x_2)+\"\\t\"+str(y))\n",
    "\n",
    "\n",
    "f.close()\n",
    "g.close()\n",
    "print (\"ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Problema de clasificación XOR:}$ El problema XOR (\"o exclusiva\") es uno de los  problemas más conocidos que no puede resolver el Perceptron, ya que es problema no linealmente separable (que exista una línea (o hiperplano) que separe grupos de entradas que tengan la misma salida). El perceptrón (de Rosemblatt) da lugar a una frontera lineal, ya que sólo entrega una combinación lineal (a lo más con la adición de una constante (bias)) de los inputs.\n",
    "\n",
    "Al introducir capas al Perceptron, se logra superar la barrera de la linealidad y es posible trazar las fronteras entre ambos conjuntos (cruces y círculos)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) Neurona artificial individual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "unit_step = lambda x: 0 if x < 0.5 else 1\n",
    "\n",
    "f = open('train.txt', 'r')\n",
    "g = open('test.txt', 'r')\n",
    "#nn = 1000\n",
    "array_input = []\n",
    "array_output = []\n",
    "\n",
    "for line in f.readlines():\n",
    "    arr = []\n",
    "    for i in line.split():\n",
    "        # i = float(i)\n",
    "        if (i != 1 and i != 0):\n",
    "            arr.append(i)\n",
    "        else:\n",
    "            array_output.append(int(i))\n",
    "    array_input.append(arr)\n",
    "\n",
    "training_set_inputs = np.array(array_input)\n",
    "training_set_outputs = np.array(array_output).T\n",
    "\n",
    "training_set_outputs = training_set_outputs.reshape(nn,1)\n",
    "\n",
    "array_input = []\n",
    "array_output = []\n",
    "\n",
    "for line in g.readlines():\n",
    "    arr = []\n",
    "    for i in line.split():\n",
    "        i = float(i)\n",
    "        if (i != 1 and i != 0):\n",
    "            arr.append(i)\n",
    "        else:\n",
    "            array_output.append(int(i))\n",
    "    array_input.append(arr)\n",
    "\n",
    "\n",
    "testing_set_inputs = np.array(array_input)\n",
    "testing_set_outputs = np.array(array_output).T\n",
    "\n",
    "testing_set_outputs = testing_set_outputs.reshape(nn,1)\n",
    "\n",
    "\n",
    "f.close()\n",
    "g.close()\n",
    "\n",
    "rd.seed(1)\n",
    "\n",
    "#Entrenamiento\n",
    "synaptic_weights = random.random((2, 1))\n",
    "h = dot(training_set_inputs, synaptic_weights)\n",
    "\n",
    "for iteration in range(10000):\n",
    "    output = 1 / (1 + np.exp(-(dot(training_set_inputs, synaptic_weights)))) # activation : Sigmoid function\n",
    "    error = training_set_outputs - output # error\n",
    "    sigmoid_derivative = output * (1 - output)\n",
    "    adjustment = dot(training_set_inputs.T,  error* sigmoid_derivative)\n",
    "    synaptic_weights += adjustment\n",
    "\n",
    "#Evaluando con el training_set\n",
    "out = 1 / (1 + np.exp(-(dot(training_set_inputs, synaptic_weights))))\n",
    "diff =  training_set_outputs - out\n",
    "cont = 0\n",
    "for i in diff:\n",
    "    j = int(mt.fabs(np.around(i, decimals=0)))\n",
    "    if(j!=0):\n",
    "        cont = cont + 1\n",
    "\n",
    "print (\"errores cometidos en la prediccion de training set: \",cont/10,\"%\")\n",
    "\n",
    "#Evaluando con el testing_set\n",
    "out = 1 / (1 + np.exp(-(dot(testing_set_inputs, synaptic_weights))))\n",
    "diff =  testing_set_outputs - out\n",
    "cont = 0\n",
    "for i in diff:\n",
    "    j = int(mt.fabs(np.around(i, decimals=0)))\n",
    "    if(j!=0):\n",
    "        cont = cont + 1\n",
    "\n",
    "print (\"errores cometidos en la prediccion de testing set: \",cont/10,\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los resultados obtenidos reflejan que el perceptrón al trazar una frontera lineal entre ambos conjuntos, termina errando en la mitad de las clasificiones, siendo esta la recta que divide de forma diagonal el conjunto de datos (ya que es está la que crea el mejor equilibrio entre error y acierto). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) Perceptron multicapa (\"hecho a mano\"): Preparación de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = open('train.txt', 'r')\n",
    "g = open('test.txt', 'r')\n",
    "\n",
    "array_data = []\n",
    "\n",
    "for line in f.readlines():\n",
    "    arr = []\n",
    "    array_data1 = []\n",
    "    line = line.split(\" \")\n",
    "    arr.append(float(line[0]))\n",
    "    arr.append(float(line[1]))\n",
    "    array_data1.append(np.array(arr))\n",
    "    array_data1.append(int(line[2]))\n",
    "    \n",
    "    array_data.append(array_data1)\n",
    "    \n",
    "training_data = np.array(array_data)\n",
    "#print training_data\n",
    "\n",
    "array_data = []\n",
    "\n",
    "for line in g.readlines():\n",
    "    arr = []\n",
    "    array_data1 = []\n",
    "    line = line.split(\" \")\n",
    "    arr.append(float(line[0]))\n",
    "    arr.append(float(line[1]))\n",
    "    array_data1.append(np.array(arr))\n",
    "    array_data1.append(int(line[2]))\n",
    "    \n",
    "    array_data.append(array_data1)\n",
    "    \n",
    "testing_data = np.array(array_data)\n",
    "\n",
    "f.close()\n",
    "g.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementación del Perceptrón: Después de una serie de prubas se notó que el desempeño de este perceptrón dependía en gran medida de la inicialización de la matriz de pesos, por lo que se optó por inicializarla \"a mano\", obteniendo un nivel más bajo de errores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rd.seed(1)\n",
    "\n",
    "def log_sigmoid(beta,a1,a2):\n",
    "    return 1 / (1 + np.exp(-beta*dot(a1,a2)))\n",
    "\n",
    "def ini_rand_array(A,n,m):\n",
    "    if (n>0):\n",
    "        A=[]\n",
    "        for j in range(0,n):\n",
    "            A1=[]\n",
    "            for i in range(0,m):\n",
    "                A1.append(random.uniform(-1,1))\n",
    "            A.append(np.array(A1))\n",
    "    else:\n",
    "        A=[]\n",
    "        for i in range(0,m):\n",
    "            A.append(random.uniform(-1,1))\n",
    "    return A\n",
    "\n",
    "\n",
    "w_0=[]\n",
    "w_1=[]\n",
    "\n",
    "w_0 =np.array([[ 0.94067139 , 0.40326776], [-1.00636411, -0.40734721], [ 0.49253826, -0.24534223]])\n",
    "w_1 = np.array([ 2.32191722 ,-2.97298485, -0.45779028])\n",
    "\n",
    "eta = 0.001\n",
    "\n",
    "n = 100000\n",
    "beta = 15\n",
    "\n",
    "for iteration in xrange(n):\n",
    "    #Forward pass\n",
    "    l_0,expected = choice(training_data)\n",
    "    l_0 = np.append(l_0,1)\n",
    "    l_1 = log_sigmoid(beta,l_0,w_0) \n",
    "    l_1 = np.append(l_1,1)\n",
    "    l_2 = log_sigmoid(beta,l_1,w_1)\n",
    "    \n",
    "    #Backward pass\n",
    "    #Segunda capa\n",
    "    delta_1 = ( expected - l_2) * l_2*(1-l_2)\n",
    "    w_1 += eta*l_1*delta_1 #Ajuste de pesos\n",
    "    #Primera Capa\n",
    "    delta_0 = dot(w_1,delta_1) * (l_1*(1-l_1))\n",
    "    w_0 = (w_0.T + eta*l_0*delta_0).T#Ajuste de pesos\n",
    "\n",
    "cont = 0\n",
    "\n",
    "#Evaluando resultados con el testing_set\n",
    "for l_0, exp in testing_data:\n",
    "    l_0 = np.append(l_0,1)\n",
    "    l_1 = np.append(log_sigmoid(beta,l_0.T,w_0),1)\n",
    "    result = log_sigmoid(beta,l_1.T,w_1)\n",
    "\n",
    "    if(unit_step(result)!=exp):\n",
    "        cont+= 1\n",
    "\n",
    "print \"errores cometidos en la prediccion:\", cont/10,\"%\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr><hr><hr><hr><hr><hr><hr><hr>\n",
    "## 3 (a) Contrucción de las matrices $X_{tr}$, $Y_{tr}$, $X_e$, $Y_e$, $X_{te}$ e $Y_{te}$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def unpickle(file):\n",
    "    #abre el archivo\n",
    "    fo = open(file, 'rb')\n",
    "    #descomprime el archivo y lo guarda en dict\n",
    "    dict = pickle.load(fo)\n",
    "    #cierra el archivo\n",
    "    fo.close()\n",
    "    #retorna dict\n",
    "    return dict\n",
    "\n",
    "\n",
    "#Funcion encode: codifica el output Y como vectores de dim 10, con cada dimensión representando\n",
    "#una categoria del problema.\n",
    "def encode(Y):\n",
    "    for i in range(len(Y)):\n",
    "        j=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "        j[Y[i]-1]=1\n",
    "        Y[i]=np.array(j)\n",
    "    return np.array(Y)\n",
    "\n",
    "\n",
    "\n",
    "#Nombre de labels\n",
    "label_names = ['airplane',  #0\n",
    "               'automobile',#1\n",
    "               'bird',      #2\n",
    "               'cat',       #3\n",
    "               'deer',      #4\n",
    "               'dog',       #5\n",
    "               'frog',      #6\n",
    "               'horse',     #7\n",
    "               'ship',      #8\n",
    "               'truck']     #9\n",
    "\n",
    "\n",
    "def load_CIFAR_one(filename):\n",
    "    #abre el archivo data_ o test_\n",
    "    with open(filename, 'rb') as f:\n",
    "        datadict = pickle.load(f)\n",
    "        \n",
    "        #matriz de 10000 x 3072 (np.array) [c/fila es una imagen]\n",
    "        X1 = datadict['data']\n",
    "        \n",
    "        #divide X1 en 9000 de entrenamiento y 1000 para validation set.\n",
    "        X2 = X1[9000:10000]  # Entrenamiento\n",
    "        X1 = X1[:9000]       # Validación\n",
    "        \n",
    "        # lista de 10000 valores enteros en [0,9] que identifica las clases\n",
    "        Y1 = datadict['labels']\n",
    "        \n",
    "        # divide las labels correspondientes para training y validation set.\n",
    "        Y2 = Y1[9000:10000]   # Entrenamiento\n",
    "        Y1 = Y1[:9000]        # Validación\n",
    "        \n",
    "        # Codifica ambos conjuntos de labels\n",
    "        Y1 = encode(Y1)\n",
    "        Y2 = encode(Y2)\n",
    "\n",
    "        #retorna por separado data y labels\n",
    "        return X1, Y1, X2 , Y2\n",
    "\n",
    "# Funcion analoga para el conjunto de test.    \n",
    "def load_CIFAR_batch(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        datadict = pickle.load(f)\n",
    "        Xte = datadict['data']\n",
    "        Yte = datadict['labels']\n",
    "        Yte = encode(Yte)\n",
    "        return Xte, Yte\n",
    "\n",
    "#Funcion que prepara las matrices pedidas juntando los 5 data_batch y extreae el validation set.    \n",
    "def load_CIFAR10(PATH):\n",
    "    xs = []\n",
    "    ys = []\n",
    "    xe = []\n",
    "    ye = []\n",
    "    \n",
    "    for b in range(1,6):\n",
    "        #abre el archivo data_batch\n",
    "        f = os.path.join(PATH, 'data_batch_%d' % (b, ))\n",
    "        \n",
    "        # separa en X1 e Y1 data y labels del diccionario abierto para training\n",
    "        # y en X2 e Y2 los de validation.\n",
    "        X1,Y1,X2,Y2 = load_CIFAR_one(f)\n",
    "        \n",
    "        # guada c/u (y todos) en las listas xs e ys los de training y en xe e ye los de validation\n",
    "        xs.append(X1)   # Training  \n",
    "        ys.append(Y1)   # Training\n",
    "        xe.append(X2)   # Validación\n",
    "        ye.append(Y2)   # Validación\n",
    "        \n",
    "    # crea un array gigante con todos los datos de entrenamiento\n",
    "    Xtr = np.concatenate(xs)\n",
    "    Ytr = np.concatenate(ys)\n",
    "    \n",
    "    # crea un array gigante con todos los datos de validation\n",
    "    Xe  = np.concatenate(xe)\n",
    "    Ye  = np.concatenate(ye) \n",
    "    del X1, Y1, X2, Y2\n",
    "    \n",
    "    #crea array de testing\n",
    "    Xte, Yte = load_CIFAR_batch(os.path.join(PATH, 'test_batch'))\n",
    "    return Xtr, Ytr, Xe, Ye, Xte, Yte\n",
    "\n",
    "\n",
    "Xtr, Ytr, Xe, Ye, Xte, Yte = load_CIFAR10(\".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) Funcion que escala las imágenes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def scale_img(X,Center=True,Scale=True):\n",
    "    # with_mean = False disable centering\n",
    "    # with_std = False disable scale\n",
    "    scaler = StandardScaler(with_mean=Center,with_std=Scale).fit(X)\n",
    "    X_scaled = pd.DataFrame(scaler.transform(X),X[:,0])\n",
    "    return X_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c) Redes neuronales con salida $\\textit{softmax}$.\n",
    "\n",
    "Arquitecturas probadas (número de neuronas ocultas por capa) y error de clasificación obtenido sobre el conjunto de validación:\n",
    "- 10-20-10  Error: 57.76%\n",
    "- 50-100-10 Error: 54.14%\n",
    "- 100-500-10 Error: 55%\n",
    "- 100-500-100-10 Error: 52.82% (92% de acc. sobre training set)\n",
    "- 100-500-500-100-10 Error: 50.02% (100% acc. sobre training set)\n",
    "- 100-100-100-100-100-10 Error: 50.16% \n",
    "- 50-100-100-100-100-10 Error: 49.92% (100% acc. sobre training set)\n",
    "- 100-200-300-200-100-10 Error: 47.85% (100% acc. sobre training set)\n",
    "\n",
    "Se probó con distintas combinaciones de funciones de activación pero las mejores siempre fueron usando exclusivamente 'relu' (al introducir 'sigmoid' inmediatamente bajaba el acc. del modelo).\n",
    "\n",
    "Learning rate se varió entre $0.02$ y $0.001$ para las primeras 2 redes, los mejores resultados fueron con $0.009$ dejando ese valor fijo en adelante.\n",
    "\n",
    "#### Se muestra a continuación el mejor modelo encontrado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rd.seed(1)\n",
    "\n",
    "#Modelo\n",
    "def model(Xtr,Ytr,Xe,Ye,Xte,Yte):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(100, input_dim=Xtr.shape[1], init='uniform'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(200, init='uniform'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(300, init='uniform'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(200, init='uniform'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(100, init='uniform'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(output_dim=10, init='uniform'))\n",
    "    model.add(Activation('softmax'))\n",
    "    sgd = SGD(lr=0.0009)\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "    hist = model.fit(np.asmatrix(Xtr), \n",
    "                     np.asmatrix(Ytr), \n",
    "                     nb_epoch=300,\n",
    "                     verbose=0, \n",
    "                     validation_data=(np.asmatrix(Xe), \n",
    "                                      np.asmatrix(Ye)\n",
    "                                     )\n",
    "                    )\n",
    "    score = model.evaluate(np.asmatrix(Xe),np.asmatrix(Ye))\n",
    "    print(\"Baseline Error: %.2f%%\" % (100-score[1]*100))\n",
    "    \n",
    "\n",
    "    score = model.evaluate(np.asmatrix(Xte),np.asmatrix(Yte))\n",
    "    print(\"Baseline Error Test: %.2f%%\" % (100-score[1]*100))\n",
    "    return hist\n",
    "\n",
    "model(Xtr, Ytr, Xe, Ye, Xte, Yte)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Error obtenido en Validation set: 47.85%\n",
    "\n",
    "Error obtenido sobre testing set: 47.33%\n",
    "\n",
    "\n",
    "(d) Utilizando el mejor modelo obtenido en parte $(c)$ con los diferentes $extract\\_features$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#extrae hog features\n",
    "print 'hog_features'\n",
    "model(extract_features(Xtr,[hog_features]),Ytr,extract_features(Xe,[hog_features]),Ye,extract_features(Xte,[hog_features]),Yte) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Error obtenido en Validation set: 49.42%\n",
    "\n",
    "Error obtenido sobre testing set: 50.17%\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#extrae histogramas de color\n",
    "print 'color_histogram'\n",
    "model(extract_features(Xtr,[color_histogram_hsv]),Ytr,extract_features(Xe,[color_histogram_hsv]),Ye,extract_features(Xte,[color_histogram_hsv]),Yte) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Error obtenido en Validation set: 84.37%\n",
    "\n",
    "Error obtenido sobre testing set: 83.48%\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#extrae todo\n",
    "print 'hog_features&color_histogram'\n",
    "model(extract_features(Xtr,[hog_features, color_histogram_hsv]),Ytr,extract_features(Xe,[hog_features, color_histogram_hsv]),Ye,extract_features(Xte,[hog_features, color_histogram_hsv]),Yte) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Error obtenido en Validation set: 46.95%\n",
    "\n",
    "Error obtenido sobre testing set: 46.59%\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:python35]",
   "language": "python",
   "name": "conda-env-python35-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
