{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <h1>INF-477. Redes Neuronales Artificiales.</h1>\n",
    "    <h2>Tarea 1 - Perceptrones Multicapa ó Redes FF</h2>\n",
    "    <hr>\n",
    "    <h3>Juan Carlos Garcés Bernt</h3>\n",
    "    <h5>jcgarces@alumnos.inf.utfsm.cl</h5>\n",
    "<center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 El XOR\n",
    "<img width=\"20%\" src=\"./img/deseado-XOR.jpg\">\n",
    "<center>Fig. 1: Distribución deseada para la actividad 1. Los 2 colores representan 2 clases distintas.</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Escriba una función que genere (aleatoriamente) $n$ datos etiquetados de la forma $\\{(x_{1}, y_{1}),...,(x_{n}, y_{n})\\}$,\n",
    "$x_{i} \\in \\mathbb{R}^{2}, y_{i} \\in \\{0, 1\\}$, con una distribución de probabilidad que refleje la configuración linealmente inseparable que muestra la $Fig.1$. Utilice esta función para generar un conjunto de $1000$ datos de entrenamiento y $1000$ datos de pruebas. El problema de clasificación obtenido se denomina en ocasiones\n",
    "\"XOR\". ¿Porqué?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from ipywidgets import *\n",
    "from numpy.random import uniform\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "n=1000\n",
    "x_train = uniform(low=-1.0,high=1.0,size=(n,2))\n",
    "x_test = uniform(low=-1.0,high=1.0,size=(n,2))\n",
    "\n",
    "y_train=[]\n",
    "y_test=[]\n",
    "c1x=[]\n",
    "c1y=[]\n",
    "c2x=[]\n",
    "c2y=[]\n",
    "\n",
    "# Tag TRAIN\n",
    "# shape[0] cantidad de filas\n",
    "for i in range(x_train.shape[0]):\n",
    "    if x_train[i,0]>0 and x_train[i,1]>0:\n",
    "        y_train.append(1)\n",
    "        c1x.append(x_train[i,0])\n",
    "        c1y.append(x_train[i,1])\n",
    "\n",
    "    else: \n",
    "        if x_train[i,0]<0 and x_train[i,1]<0:\n",
    "            y_train.append(1)\n",
    "            c1x.append(x_train[i,0])\n",
    "            c1y.append(x_train[i,1])\n",
    "        else:\n",
    "            y_train.append(0)\n",
    "            c2x.append(x_train[i,0])\n",
    "            c2y.append(x_train[i,1])\n",
    "\n",
    "\n",
    "for i in range(x_test.shape[0]):\n",
    "    if x_test[i,0]>0 and x_test[i,1]>0:\n",
    "        y_test.append(1)\n",
    "\n",
    "    else: \n",
    "        if x_test[i,0]<0 and x_test[i,1]<0:\n",
    "            y_test.append(1)\n",
    "        else:\n",
    "            y_test.append(0)\n",
    "\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.plot(c1x, c1y, 'go',label=\"Clase 1\")\n",
    "plt.plot(c2x, c2y, 'rx', label=\"Clase 2\")\n",
    "#plt.axhline(0, color='k')\n",
    "#plt.axvline(0, color='k')\n",
    "#plt.legend(loc='center right')\n",
    "plt.axis([-1.05, 1.05, -1.05, 1.05])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Demuestre experimentalmente que una neurona artificial individual no puede resolver satisfactoriamente el problema anterior. Puede utilizar la funcion de activación y el método de entrenamiento que prefiera. Sea convincente. Describa y explique lo que observa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(1, input_dim=1000))\n",
    "model.add(Activation('relu'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Demuestre experimentalmente que un perceptron multicapas puede resolver satisfactoriamente el problema obtenido en (a). Puede utilizar la arquitectura y el método de entrenamiento que prefiera. Sea convincente. Describa y explique lo que observa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "model = Sequential() # nuevo modelo tipo sequential (definida por capas)\n",
    "\n",
    "model.add(Dense(15, input_dim=2, init=\"uniform\"))\n",
    "model.add(Activation(\"relu\")) # a la capa escondida\n",
    "model.add(Dense(1, init=\"uniform\")) #output\n",
    "model.add(Activation(\"linear\")) # lineal       softmax\n",
    "\n",
    "sgd = SGD(lr=0.8)\n",
    "\n",
    "# funcion de pérdida: 0.1, eqm,    problema de clasificación o regresión mean_squared_error binary_crossentropy\n",
    "model.compile(optimizer=sgd,loss=\"mean_squared_error\")#, metrics=[\"accuracy\"])\n",
    "\n",
    "# model.fit -> entrenar\n",
    "hist = model.fit(x_train,\n",
    "                 y_train, \n",
    "                 nb_epoch=300,\n",
    "                 verbose=0)\n",
    "error_test = model.evaluate(x_test,y_test)\n",
    "error_train = model.evaluate(x_train,y_train)\n",
    "print(\"\\nTest:\",error_test)\n",
    "print(\"Train: \",error_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr><hr><hr>\n",
    "# 2 Predicción de Precio de una Casa\n",
    "En esta sección trabajaremos con un pequeño dataset conocido como *Boston Housing* que nos permitirá\n",
    "experimentar de modo más completo y exhaustivo con las técnicas bajo estudio. El problema consiste en\n",
    "predecir el precio de una casa en una zona/barrio de Boston (USA) a partir de una serie de atributos que\n",
    "describen el lugar que éste se ubica: tasa de criminalidad, proporción de zona residencial, proporción de zona\n",
    "industrial, si se encuentra junto al río ó no, contaminación atmosférica medida como la concentración de\n",
    "óxidos nítricos en el aire, etc. Para ver en detalle la descripción de la semántica asociada a los atributos de\n",
    "este problema, puede consultar https://archive.ics.uci.edu/ml/datasets/Housing.\n",
    "<img width=\"70%\" src=\"./img/houses.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a) Construya un dataframe con los datos a analizar descargando los datos desde la URL mantenida por los autores de . Explique qué hacen las líneas $4$ a $7$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "url = \"http://mldata.org/repository/data/download/csv/regression-datasets-housing/\"\n",
    "\n",
    "df = pd.read_csv(url, sep=\",\",header=None, names=[\"CRIM\",\"ZN\",\"INDUS\",\"CHAS\",\"NOX\",\n",
    "        \"RM\",\"AGE\",\"DIS\",\"RAD\",\"TAX\",\"PTRATIO\",\"B\",\"LSTAT\",\"MEDV\"])\n",
    "from sklearn.cross_validation import train_test_split\n",
    "df_train, df_test = train_test_split(df, test_size=0.25, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la línea 4 y 5 se crea una variable df (dataframe) que hace lectura de un documento separado por comas (.CSV) ignorando la cabecera (header) y todas las columnas correspondientes a la lista \"names\", las siglas se pueden ver a continuación\n",
    "Attribute Information:\n",
    "\n",
    "1. **CRIM**: per capita crime rate by town \n",
    "2. **ZN**: proportion of residential land zoned for lots over 25,000 sq.ft. \n",
    "3. **INDUS**: proportion of non-retail business acres per town \n",
    "4. **CHAS**: Charles River dummy variable (= 1 if tract bounds river; 0 otherwise) \n",
    "5. **NOX**: nitric oxides concentration (parts per 10 million) \n",
    "6. **RM**: average number of rooms per dwelling \n",
    "7. **AGE**: proportion of owner-occupied units built prior to 1940 \n",
    "8. **DIS**: weighted distances to five Boston employment centres \n",
    "9. **RAD**: index of accessibility to radial highways \n",
    "10. **TAX**: full-value property-tax rate per \\$10,000 \n",
    "11. **PTRATIO**: pupil-teacher ratio by town \n",
    "12. **B**: 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town \n",
    "13. **LSTAT**: % lower status of the population \n",
    "14. **MEDV**: Median value of owner-occupied homes in \\$1000's\n",
    "\n",
    "En la línea 6 se importa la función *train_test_split* de la librería de *sklearn.cross_validation*, esta función toma como input un dataframe y por medio de parámetros puede manejar dicha información retornando dos dataframes (entrenamiento y pruebas) , en este caso se está utilizando el parámetro: \n",
    "\n",
    "En la línea 7 se deinen dos variables *df_train* y *df_test* que serán los 2 nuevos dataframes obtenidos por la división del original (df) al utilizar la función recien importada *train_test_split*, esta función recibe tres parámetros de entrada:\n",
    "\n",
    "*df*: Datafrase obtenido desde el archivo .CSV\n",
    "\n",
    "*test_size* : Corresponde a un valor entre 0 y 1 que representa la porporción del dataset que incluirá en la separación del dataset. \n",
    "\n",
    "*random_state* : Numero generador para un muestreo aleatorio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (b) Describa brevemente el dataset a utilizar\n",
    "```python\n",
    "df.shape\n",
    "df.info()\n",
    "df.describe()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Función para describir el dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def DescribeDataSet(df):\n",
    "    r,c = df.shape\n",
    "    print(\"Dataframe de {} filas y {} columnas\\n\\n\".format(r,c))\n",
    "\n",
    "    print(\"Resumen del dataframe:\")\n",
    "    df.info()\n",
    "    print(\"\\n\\n\")\n",
    "\n",
    "    print(\"Dataframe (sin datos nulos):\")\n",
    "    print(df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dataframe : df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DescribeDataSet(df_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dataframe : df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DescribeDataSet(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (c) Normalice los datos antes de trabajar. explique la importancia/conveniencia de realizar esta operación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler().fit(df_train)\n",
    "X_train_scaled = pd.DataFrame(scaler.transform(df_train),columns=df_train.columns)\n",
    "y_train = X_train_scaled.pop(\"MEDV\")\n",
    "\n",
    "X_test_scaled = pd.DataFrame(scaler.transform(df_test),columns=df_test.columns)\n",
    "y_test = X_test_scaled.pop(\"MEDV\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (d) Muestre en un gráfico el error cuadrático (MSE) vs número de *epochs* de entrenamiento, para una red *feedforward* de $3$ capas, con $200$ unidades ocultas y función de activación *sigmoidal*. Entrene la red usando gradiente descendente estocástico con *learning rate* $0.2$ y $300$ epochs de entrenamiento, en el conjunto de entrenamiento y de test. Comente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "def plotMseVsEpoch(h_layer,activation,learning_rate,epoch):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(h_layer, input_dim=X_train_scaled.shape[1], init=\"uniform\"))\n",
    "    model.add(Activation(activation))\n",
    "    model.add(Dense(1, init=\"uniform\"))\n",
    "    model.add(Activation(\"linear\"))\n",
    "\n",
    "    sgd = SGD(lr=learning_rate)\n",
    "    model.compile(optimizer=sgd,loss=\"mean_squared_error\")\n",
    "\n",
    "    hist = model.fit(X_train_scaled.as_matrix(), y_train.as_matrix(), \n",
    "                     nb_epoch=epoch, \n",
    "                     verbose=0, \n",
    "                     validation_data=(X_test_scaled.as_matrix(), \n",
    "                                      y_test.as_matrix()\n",
    "                                     )\n",
    "                    )\n",
    "    plt.plot(hist.epoch,hist.history['loss'],'-r',label=activation)\n",
    "    plt.figure(figsize=(20,5))\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.suptitle('Epoch VS MSE ',fontsize=22)\n",
    "    plt.ylabel('MSE',fontsize=20)\n",
    "    plt.xlabel('Epoch',fontsize=20)\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plotMseVsEpoch(200,\"sigmoid\",0.01,300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (e) Repita el paso anterior, utilizado 'Relu' como función de activación y compare con lo obtenido en d)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plotMseVsEpoch(200,\"relu\",0.2,300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (f) Repita d) variando el learning rate. Comente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "n_lr = 20\n",
    "lear_rate = np.linspace(0,1,n_lr)\n",
    "for lr in lear_rate:\n",
    "    print(\"Learning rate:\",lr)\n",
    "    plotMseVsEpoch(200,\"sigmoid\",lr,300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (g) Estime el error de predicción de los modelos d) y e) usando validación cruzada con un número de folds igual a $K = 5$ y $K = 10$. Recuerde que para que la estimación sea razonable debe ajustar los pesos del modelo de nuevo, cada vez que trabaja sobre un determinado fold. Mida el error real del modelo sobre el conjunto de pruebas, compare y concluya."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import cross_validation\n",
    "\n",
    "Xm = X_train_scaled.as_matrix()\n",
    "ym = y_train.as_matrix()\n",
    "kfold = cross_validation.KFold(len(Xm), 10)\n",
    "cvscores = []\n",
    "for i, (train, val) in enumerate(kfold):\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(200, input_dim=Xm.shape[1], init=\"uniform\"))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(Dense(1, init=\"uniform\"))\n",
    "    model.add(Activation(\"linear\"))\n",
    "    # Compile model\n",
    "    sgd = SGD(lr=0.2)\n",
    "    model.compile(optimizer=sgd,loss=\"mean_squared_error\")\n",
    "    # Fit the model\n",
    "    model.fit(Xm[train], ym[train], nb_epoch=300,verbose=0 )\n",
    "    # evaluate the model\n",
    "    scores = model.evaluate(Xm[val], ym[val])\n",
    "    cvscores.append(scores)\n",
    "\n",
    "mse_cv = np.mean(cvscores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(cvscores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (h) Entrene el modelo obtenido en d) usando *progressive decay* Compare y comente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_decay = 10\n",
    "lear_decay = np.logspace(-6,0,n_decay)\n",
    "sgd = SGD(lr=0.2, decay=1e-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (i) Entrene el modelo obtenido en d) usando momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_decay = 21\n",
    "momentum = np.linspace(0,1,n_decay)\n",
    "sgd = SGD(lr=0.2,momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (j) Entrene los modelos obtenidos en d) y e) cambiando el tamaño del batch. Compare SGD, batch y mini-batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_batches = 21\n",
    "batch_sizes = np.round(np.linspace(1,X_train_scaled.shape[0],n_batches))\n",
    "model.fit(X_train_scaled.as_matrix(), y_train_scaled.as_matrix(), batch_size=50, nb_epoch=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr><hr><hr>\n",
    "<img width=\"30%\" src=\"./img/CIFAR10.jpg\">\n",
    "# 3 Reconocimiento de Imágenes en CIFAR10\n",
    "En esta sección trabajaremos con un dataset bastante conocido y utilizado por la comunidad para experimentar\n",
    "con reconocimiento de objetos en imágenes: **CIFAR10**. Se trata de un conjunto de $60.000$ imágenes\n",
    "RGB de $32 \\times 32$ pixeles que contiene $10$ clases de objetos y $6000$ ejemplos por clase. La versión utilizada\n",
    "se atribuye a A. Krizhevsky, V. Nair y G. Hinton $[3]$ y viene separada en $50000$ ejemplos de entrenamiento\n",
    "y $10000$ casos de prueba. El conjunto de pruebas fue obtenido seleccionando $1000$ imágenes aleatorias de\n",
    "cada clase. Los datos restantes han sido ordenados aleatoriamente y están organizados en $5$ bloques de entrenamiento\n",
    "(batches). Las clases son mutuamente excluyentes y corresponden a las siguientes categorías:\n",
    "gatos, perros, ranas, caballos, pájaros, ciervos, aviones, automóviles, camiones y barcos.\n",
    "Los datos asociados a esta actividad podrán ser obtenidos utilizando los siguientes comandos en la línea\n",
    "de comandos (sistemas UNIX)\n",
    "```python\n",
    "wget http://octopus.inf.utfsm.cl/~ricky/data.tar.gz\n",
    "tar -xzvf data.tar.gz\n",
    "rm data.tar.gz\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:python35]",
   "language": "python",
   "name": "conda-env-python35-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
